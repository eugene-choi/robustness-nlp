# Implementing Continual Robustness Gym Evaluation on Pre-trained Models
Project for DS-UA 203 - LING-UA 52: Machine Learning for Language Understanding, taught by Prof. Samuel Bowman, Spring 2021.

##Motivation
Despite the recent successes in attaining near-human accuracy post BERT, many state-of-the-art NLP models performed worse when deployed in the real world. This is primarily due to the fundamental assumption that both training and test data are independent and identically distributed, which all machine learning models rely upon (Bishop et al., 2006). However, this assumption hardly holds in reality since the data is often corrupted, and its distribution shifts constantly. This unforeseen shift not only degrades the performance of the model but also leads to severe social consequences such as the amplification of social bias and discriminations. There are two ways of addressing this issue: the first method is to make a novel, weaker assumption to replace the i.i.d. assumption with greater generalization ability. Though this would be the direct way of addressing the problem, this is an out-of-scope task given our limited skills; the second approach is to address the issue through thorough testing. To mitigate this problem, the Robustness Gym (Goel et al., 2021) suggests a shift from static to continual evaluation to help test NLP models and consider robustness issues. Since the paper has only explored either commercial or generative models, for our project, we propose to contribute to this novel NLP evaluation landscape by running experiments using state-of-the-art models that had not been experimented on in the paper, then evaluating robustness through four paradigms: Evaluation Sets, Subpopulations, Transformations, and Attacks. 

##Data Collection
We will be using the SQuAD 2.0 dataset and the Multi-Genre Natural Language Inference (MultiNLI) corpus. Both datasets can be accessed through Huggingface. 

##Modeling and Analysis
Using the Huggingface library, we will be focusing on the benchmark and state-of-the-art pre-trained models in classification tasks. We will be using BERT (Devlin et al. 2019), DeBERTA (He et al., 2021), RoBERTA (Liu et al., 2019), T5, and TransformerXL fine-tuning on tasks such as question answering tasks using the SQuAD v2 dataset and Natural Language Inference task using the Multi-Genre Natural Language Inference (MultiNLI) corpus. 
We will perform our analysis using the “slice” function implemented in the Robustness Gym python toolkit. The “slice” function allows us to divide our dataset into subparts, or the four paradigms, that will help us better interpret our model performances. 
